<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta
            http-equiv="Content-Security-Policy"
            content="upgrade-insecure-requests"
    />

    <title>Dendritic Learner - Viet Anh Khoa Tran</title>
    <meta
            name="description"
            content="Hi there! I am interested in how the biological brain updates,
        represents and reasons upon information, and to apply that knowledge to
        build artificial learning agents (NeuroAI)."
    />

    <link rel="preconnect" href="https://fonts.googleapis.com"/>
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin/>
    <link
            href="https://fonts.googleapis.com/css2?family=IBM+Plex+Serif:ital,wght@0,300;0,400;0,500;0,600;0,700;1,400&display=swap"
            rel="stylesheet"
    />

    <link
            href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.6/dist/css/bootstrap.min.css"
            rel="stylesheet"
            integrity="sha384-4Q6Gf2aSP4eDXB8Miphtr37CMZZQ5oXLH2yaXMJ2w8e2ZtHTl7GptT4jmndRuHDT"
            crossorigin="anonymous"
    />
    <link href="css/custom.css" type="text/css" rel="stylesheet"/>
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml"/>
</head>
<body>
<header class="container title-container">
    <div class="row mb-3">
        <div class="col-lg-8">
            <span class="title">Dendritic Learner</span>
            <br/>
            <span class="myname">Viet Anh Khoa Tran <sup>1,2</sup></span>
            <br/>
            <span class="myaffiliations"
            ><sup>1</sup>Dendritic Learning Group (PGI-15), Forschungszentrum Jülich<br/>
                <sup>2</sup>RWTH Aachen University</span
            >
        </div>
    </div>
    <div class="row">
        <div class="col-lg-8 socials d-grid gap-2 d-lg-block">
            <a
                    class="social btn btn-primary gscholar"
                    href="https://scholar.google.com/citations?user=f0rAWkYAAAAJ&hl=en"
            ><i class="fa-brands fa-google-scholar"></i> Google Scholar</a
            >
            <a
                    class="social btn btn-primary linkedin"
                    href="https://www.linkedin.com/in/viet-anh-khoa-tran/"
            ><i class="fa-brands fa-linkedin"></i> LinkedIn</a
            >
            <a
                    class="social btn btn-primary github"
                    href="https://github.com/tran-khoa"
            ><i class="fa-brands fa-github"></i> GitHub</a
            >
            <a
                    class="social btn btn-primary bluesky"
                    href="https://bsky.app/profile/ktran.de"
            ><i class="fa-brands fa-bluesky"></i> BlueSky</a
            >
            <a class="social btn btn-primary cv" href="assets/cv.pdf"
            ><i class="fa-solid fa-bars-staggered"></i> CV</a
            >
        </div>
    </div>

</header>
<main>
    <section class="abstract">
        <div class="container">
            <div class="row">
                <div class="col-lg-8">
                    <p>
                        Hi there! I am interested in how the biological brain updates,
                        represents and reasons upon information, and to apply that knowledge
                        to build artificial learning agents (<a
                            href="https://www.nature.com/articles/s41467-023-37180-x"
                    >NeuroAI</a
                    >). My goal is to engineer machine learning algorithms and
                        architectures that learn efficiently on neuromorphic hardware and
                        reason beyond chains of thoughts hidden in massive language data.
                        Currently, I am privileged to be able to work towards that goal as a
                        PhD student at the
                        <a href="https://www.fz-juelich.de/en/pgi/pgi-15/research/dlg"
                        >Dendritic Learning Group</a
                        >
                        headed by
                        <a href="https://www.linkedin.com/in/willem-wybo-a2b53217b"
                        >Willem Wybo</a
                        >
                        as part of
                        <a href="https://www.fz-juelich.de/en/pgi/pgi-15"
                        >Emre Neftci's Neuromorphic Computing lab</a
                        >
                        at Forschungszentrum Jülich, Germany.
                    </p>
                    <p>
                        Apart from work, I enjoy playing classical piano, table tennis,
                        volleyball, bouldering, video games, and engaging myself in
                        politics, philosophy, and languages.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section class="header">
        <div class="container">
            <h2>Research</h2>
        </div>
    </section>

    <section class="research">
        <article class="container">
            <a class="paper-title h3" href="https://ktran.de/papers/tmcl/">
                Contrastive Consolidation of Top-Down Modulations Achieves Sparsely
                Supervised Continual Learning
            </a>

            <span class="paper-authors">
            <span class="me-author">Viet Anh Khoa Tran</span>, Emre O. Neftci,
            Willem A. M. Wybo
          </span>

            <div class="row">
                <div class="col-md">
                    <blockquote class="blockquote tldr">
                        <p>
                            Using contrastive learning to integrate modulations into
                            feedforward weights, continually.
                        </p>
                    </blockquote>
                    <p>
                        Biological brains learn continually from a stream of unlabeled
                        data, while integrating specialized information from sparsely
                        labeled examples without compromising their ability to
                        generalize. Meanwhile, machine learning methods are susceptible
                        to catastrophic forgetting in this natural learning setting, as
                        supervised specialist fine-tuning degrades performance on the
                        original task.
                    </p>
                    <p>
                        We introduce
                        <mark>task-modulated contrastive learning (TMCL)</mark>
                        , which takes inspiration from the biophysical machinery in the
                        neocortex, using predictive coding principles to integrate
                        top-down information continually and without supervision. We
                        follow the idea that these principles build a view-invariant
                        representation space, and that this can be implemented using a
                        contrastive loss. Then, whenever labeled samples of a new class
                        occur, new affine modulations are learned that improve
                        separation of the new class from all others, without affecting
                        feedforward weights. By co-opting the view-invariance learning
                        mechanism, we then train feedforward weights to match the
                        unmodulated representation of a data sample to its modulated
                        counterparts. This introduces modulation invariance into the
                        representation space, and, by also using past modulations,
                        stabilizes it.
                    </p>
                    <p>
                        Our experiments show improvements in both class-incremental and
                        transfer learning over state-of-the-art unsupervised approaches,
                        as well as over comparable supervised approaches, using as few
                        as 1% of available labels. Taken together, our work suggests
                        that top-down modulations play a crucial role in balancing
                        stability and plasticity.
                    </p>
                </div>
                <div class="col-md-4">
                    <figure>
                        <img src="figures/tmcl.svg" alt=""/>

                        <figcaption>
                            Cortical learning (left) is characterized by the interplay
                            between top-down (orange) and feedforward (blue) processing,
                            where top-down connections impart high-level information on
                            the feedforward sensory processing pathway (top). The
                            feedforward pathway, on the other hand, learns to predict
                            neural representations of future inputs (predictive coding).
                            Translating this view to a machine learning algorithm
                            (middle), we (i) train modulations to implement high-level
                            object identification tasks as the analogue of top-down
                            inputs, while we (ii) train for view invariance over modulated
                            representations and for modulation invariance as the analogue
                            of predictive coding (top). As a consequence, high-level
                            information continually permeates into the sensory processing
                            pathway, which can be contrasted with the traditional machine
                            learning (right) approach of unsupervised pretraining for view
                            invariance (top) followed by supervised fine-tuning (bottom).
                            In this case, it is unclear how high-level information can be
                            incorporated into the sensory processing pathway to improve
                            subsequent learning.
                        </figcaption>
                    </figure>

                    <div class="paper-actions text-end">
                        <a class="btn btn-primary btn" href="https://ktran.de/papers/tmcl/"
                        ><i class="fa-solid fa-rocket"></i> Project page</a
                        >
                        <a class="btn btn-danger" href="https://arxiv.org/abs/2505.14125">
                            <i class="fa-solid fa-file-lines"></i> Preprint
                        </a>
                    </div>
                </div>
            </div>
        </article>
        <article class="container">
            <h3 class="paper-title">
                Continual learning using dendritic modulations on view-invariant
                feedforward weights
            </h3>
            <span class="paper-authors">
            <span class="me-author">Viet Anh Khoa Tran</span>, Emre O. Neftci,
            Willem A. M. Wybo
            <span class="venue">COSYNE 2024</span>
          </span>

            <div class="row">
                <div class="col-md">
                    <blockquote class="blockquote tldr">
                        <p>
                            View-invariance + supervised top-down modulations for
                            task-incremental learning
                        </p>
                    </blockquote>
                    <p>
                        The brain can learn continuously without forgetting past skills,
                        unlike traditional machine learning models that struggle with
                        continual learning. We provide a 'fast and slow learning'
                        paradigm towards solving this problem.
                    </p>
                    <p>
                        First, we observe that standard supervised learning of neural
                        networks leads to a configuration, where the network becomes
                        invariant to task-irrelevant features, i.e. also being invariant
                        to features relevant to previous tasks ('neural collapse').
                        Instead, in a <i>slow learning phase</i>, we suggest to first
                        learn feedforward weights to extract general features by proxy
                        of a view-invariance learning objective: The training signal is
                        thereby provided by smoothly moving visual stimuli, suggesting
                        object identity. The machine learning equivalent is contrastive
                        self-supervised learning, where the network is trained to be
                        invariant towards randomly generated distortions (e.g. random
                        crops, flips, etc.).
                    </p>
                    <p>
                        Yet, a neural collapse configuration in order to read out the
                        class via a linear classifier. Inspired by our previous work, we
                        train task-specific modulations - augmenting the feedfoward
                        computation - towards such a configuration (<i>fast learning</i
                    >). We show that alternating between these two phases allows in
                        a standard continual learning setup does not lead to
                        catastrophic forgetting of task-relevant features, but requires
                        keeping track of drifting class-clusters (readout healing).
                    </p>
                </div>
                <div class="col-md-4">
                    <figure>
                        <img
                                src="figures/cosyne24.svg"
                                alt="Training the whole network to solve a classification task leads to neural collapse, hindering continual learning (left). Instead, we suggest a separate self-supervised training objective to learn view-invariant features (right), upon which task-specific modulations might induce linearly separable representations."
                        />

                        <figcaption>
                            Training the whole network to solve a classification task
                            leads to neural collapse, hindering continual learning (left).
                            Instead, we suggest a separate self-supervised training
                            objective to learn view-invariant features (right), upon which
                            task-specific modulations might induce linearly separable
                            representations.
                        </figcaption>
                    </figure>

                    <div class="paper-actions text-end">
                    <a class="btn btn-secondary" href="assets/cosyne24_poster.pdf"
                    ><i class="fa-regular fa-rectangle-list"></i> Poster</a
                    >
                    </div>
                </div>
            </div>
        </article>
        <article class="container">
            <h3 class="paper-title">
                NMDA-driven dendritic modulation enables multitask representation
                learning in hierarchical sensory processing pathways
            </h3>
            <span class="paper-authors">
            Willem A. M. Wybo, Matthias C. Tsai,
            <span class="me-author">Viet Anh Khoa Tran</span>, Bernd Illing,
            Jakob Jordan, Abigail Morrison, Walter Senn
            <span class="venue">PNAS</span>
          </span>
            <div class="row">
                <div class="col-md">
                    <p>
                        The brain can learn continuously without forgetting past skills,
                        unlike traditional machine learning models that struggle with
                        continual learning. We provide a 'fast and slow learning'
                        paradigm towards solving this problem.
                    </p>
                    <p>
                        How does the brain adapt its computation to dynamically changing
                        environments and tasks? We propose that dendritic modulation is
                        a suitable candidate for these requiremets. We show in
                        biophysically realistic simulations that task-solving
                        modulations learned via a Hebbian learning rule modulated by a
                        global error signal can be used to solve multiple tasks. Towards
                        more biologically plausible machine learning, we propose
                        task-modulated contrastive learning (TMCL) as a layer-local,
                        semi-supervised, multitask learning algorithm.
                    </p>
                </div>
                <div class="col-md-4">
                    <figure>
                        <img
                                src="figures/pnas23.png"
                                alt="Training the whole network to solve a classification task leads to neural collapse, hindering continual learning (left). Instead, we suggest a separate self-supervised training objective to learn view-invariant features (right), upon which task-specific modulations might induce linearly separable representations."
                        />

                        <figcaption>
                            Task-Modulated Contrastive Learning (TMCL) is a layer-local
                            learning algorithm, which trains weights to be invariant to
                            task modulations.
                        </figcaption>
                    </figure>

                    <div class="paper-actions text-end">
                    <a
                            class="btn btn-danger"
                            href="https://www.pnas.org/doi/full/10.1073/pnas.2300558120"
                    ><i class="fa-regular fa-file-lines"></i> Journal article</a
                    >
                    </div>
                </div>
            </div>
        </article>
        <article class="container">
            <h3 class="paper-title">
                Does Joint Training Really Help Cascaded Speech Translation?
            </h3>
            <span class="paper-authors">
            <span class="me-author">Viet Anh Khoa Tran</span>, David Thulke,
            Yingbo Gao, Christian Herold, Hermann Ney
            <span class="venue">EMNLP 2022</span>
          </span>

            <div class="row">
                <div class="col-md">
                    <blockquote class="blockquote tldr">
                        <p>In-domain fine-tuning is all you need.</p>
                    </blockquote>
                    <p>
                        A simple approach to translate speech from one language to text
                        in another language is to generate a transcript using ASR
                        (automatic speech recognition) model, which is then translated
                        using a separate MT (machine translation) model, i.e. cascaded
                        speech translation. We discuss the potential benefits of
                        training these two models jointly. Our investigations highlight
                        that the benefits of such joint training suggested by previous
                        work can be explained away by in-domain fine-tuning both ASR and
                        MT models whilst using the traditional cascaded approach.
                    </p>
                    <div class="paper-actions text-end">
                        <a class="btn btn-danger" href="https://arxiv.org/abs/2210.13700"
                        ><i class="fa-regular fa-file-lines"></i> Paper</a
                        >
                    </div>
                </div>
            </div>
        </article>
        <article class="container">
            <h3 class="paper-title">
                On Sampling-Based Training Criteria for Neural Language Modeling
            </h3>
            <span class="paper-authors">
            Yingbo Gao, David Thulke, Alexander Gerstenberger,
            <span class="me-author">Viet Anh Khoa Tran</span>, Ralf Schlüter,
            Hermann Ney
            <span class="venue">INTERSPEECH 2021</span>
          </span>

            <div class="row">
                <div class="col-md">
                    <blockquote class="blockquote tldr">
                        <p>
                            Different vocabulary sampling-based training criteria are all
                            the same, except for a correction term.
                        </p>
                    </blockquote>
                    <p>
                        As the vocabulary size of language models increases, training
                        the cross-entropy loss across the entire vocabulary becomes
                        computationally expensive. However, it is unclear why certain
                        sampling-based training criteria such as noise contrastive
                        estimation (NCE) work well in practice compared to others. Here,
                        starting from three fundamental criteria, namely mean squared
                        error (MSE), binary cross-entropy (BCE), and cross-entropy (CE),
                        we explicitly write out sampling-based versions such as
                        importance sampling, NCE and standard Monte Carlo sampling and
                        derive a 'correction term' that - if applied during inference -
                        makes the sampling-based training criteria perform similarly to
                        NCE.
                    </p>
                    <div class="paper-actions text-end">
                        <a class="btn btn-danger" href="https://arxiv.org/abs/2104.10507"
                        ><i class="fa-regular fa-file-lines"></i> Paper</a
                        >
                    </div>
                </div>
            </div>
        </article>
        <article class="container">
            <h3 class="paper-title">
                Analysis of positional encodings for neural machine translation
            </h3>
            <span class="paper-authors">
            Jan Rosendahl, <span class="me-author">Viet Anh Khoa Tran</span>,
            Weiyue Wang, Hermann Ney
            <span class="venue">IWSLT 2019</span>
            </span>
            <div class="row">
                <div class="col-md">
                    <blockquote class="blockquote tldr">
                        <p>
                            Relative positional encodings help generalize to longer
                            sequences
                        </p>
                    </blockquote>
                    <p>
                        We show in the context of machine translation that while
                        relative positional encodings are not beneficial for performance
                        on sequence lengths seen during training, they are crucial for
                        generalization to longer sequences. Nowadays, this fact is
                        widely acknowledged outside of machine translation (e.g.
                        <a href="https://arxiv.org/abs/2108.12284"
                        >Csordas, Irie and Schmidhuber, 2021</a
                        >) and relative positional encodings are used in many
                        state-of-the-art models.
                    </p>
                </div>
                <div class="col-md-4">
                    <figure>
                        <img
                                src="figures/iwslt2019.svg"
                                alt="Relative positional encodings as proposed by Shaw et al., 2019"
                        />

                        <figcaption>
                            Relative positional encodings as proposed by Shaw et al., 2019
                        </figcaption>
                    </figure>
                    <div class="paper-actions text-end">
                        <a
                                class="btn btn-danger"
                                href="https://aclanthology.org/2019.iwslt-1.20/"
                        >
                            <i class="fa-solid fa-file-lines"></i> Paper
                        </a>
                    </div>
                </div>

            </div>
        </article>
    </section>

    <section class="header">
        <div class="container">
            <h2>Side projects</h2>
        </div>
    </section>

    <section class="projects container">
        <ul class="list-group list-group-flush">
            <li class="list-group-item">
                <a
                        class="project-name"
                        href="https://github.com/tran-khoa/krunner-zotero"
                >KRunner Zotero</a
                >
                <br/>
                Search through your Zotero papers based on metadata as well as
                notes, tags, etc., via KDE Plasma's awesome KRunner. Inspired by
                ZotHero.
            </li>
            <li class="list-group-item">
                <a
                        class="project-name"
                        href="https://github.com/tran-khoa/ANNO1404-Warenrechner-App"
                >ANNO 1404 Supply Chain Calculator </a
                ><br/>
                A high school side project to calculate the optimal supply chain in
                ANNO 1404 given the current population as an Android app.
            </li>
            <li class="list-group-item">
                <a
                        class="project-name"
                        href="https://github.com/lebertran/harvestcraft"
                >
                    Porting HarvestCraft to Minecraft 1.9 </a
                ><br/>
                With 90M downloads,
                <a
                        href="https://www.curseforge.com/minecraft/mc-mods/pams-harvestcraft"
                >Pam's HarvestCraft</a
                >
                is one of the most popular mods for Minecraft. I ported it to
                Minecraft 1.9 which
                <a href="https://www.patreon.com/posts/harvestcraft-1-9-5789042"
                >made it into the official code</a
                >. This is the project that started my journey into programming.
            </li>
        </ul>
    </section>
</main>

<footer>
    <div class="container">&copy; 2025 Viet Anh Khoa Tran</div>
</footer>
<script
        src="https://kit.fontawesome.com/39127df8ef.js"
        crossorigin="anonymous"
></script>
</body>
</html>
