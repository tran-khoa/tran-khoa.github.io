<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Dendritic Learner</title>
    <link href="https://latex.vercel.app/style.css" rel="stylesheet" />
    <link rel="icon" href="favicon.svg" sizes="any" type="image/svg+xml" />
    <style>
      body {
        min-width: 100ch;
      }
      a {
        text-decoration: none;
      }
      .bold {
        font-weight: bold;
      }
      a:hover {
        text-decoration: underline;
      }
      ul.articles > li {
        margin-bottom: 1rem;
      }
      p {
        margin-top: 0;
      }
      .social-sep {
        margin: 0 4px;
      }
      .paper_title {
        font-weight: bold;
        font-size: 1.2rem;
        line-height: 1rem;
      }
      .paper_authors {
        display: block;
        font-style: italic;
        font-size: 0.8rem;
      }
      .paper_venue {
        display: block;
        font-size: 1rem;
      }
      .paper_venue::before {
        content: " published at ";
        color: gray;
        font-size: 0.8rem;
      }
      figure {
        text-align: center;
      }
      figure > img {
        margin: auto;
      }
      figure > figcaption {
        margin-top: 8px;
      }
    </style>
  </head>
  <body class="libertinus">
    <header>
      <h1>Dendritic Learner</h1>
      <p class="author">
        Viet Anh Khoa Tran <br />
        <a class="social" href="https://www.linkedin.com/in/viet-anh-khoa-tran/"
          ><i class="fa-brands fa-linkedin"></i> LinkedIn</a
        >
        <span class="social-sep"></span>
        <a
          class="social"
          href="https://scholar.google.com/citations?user=f0rAWkYAAAAJ&hl=en"
          ><i class="fa-brands fa-google-scholar"></i> Google Scholar</a
        >
        <span class="social-sep"></span>
        <a class="social" href="https://github.com/tran-khoa"
          ><i class="fa-brands fa-github"></i> GitHub</a
        >
        <span class="social-sep"></span>
        <a class="social" href="assets/cv.pdf"
          ><i class="fa-solid fa-bars-staggered"></i> CV</a
        >
      </p>
    </header>

    <div class="abstract">
      <h2>Abstract</h2>
      <p>
        Hi there! I am interested in how the biological brain updates,
        represents and reasons upon information, and to apply that knowledge to
        build artificial learning agents (<a
          href="https://www.nature.com/articles/s41467-023-37180-x"
          >NeuroAI</a
        >). My goal is to engineer machine learning algorithms and architectures
        that learn efficiently on neuromorphic hardware and reason beyond chains
        of thoughts hidden in massive language data. Currently, I am privileged
        to be able to work towards that goal as a PhD student at the
        <a href="https://www.fz-juelich.de/en/pgi/pgi-15/research/dlg"
          >Dendritic Learning Group</a
        >
        headed by
        <a href="https://www.linkedin.com/in/willem-wybo-a2b53217b"
          >Willem Wybo</a
        >
        as part of
        <a href="https://www.fz-juelich.de/en/pgi/pgi-15">Emre Neftci's Lab</a>
        at Forschungszentrum Jülich, Germany.
      </p>
      <p>
        Apart from work, I enjoy playing classical piano, table tennis,
        volleyball, bouldering, video games, and engaging myself in politics,
        philosophy, and languages.
      </p>
    </div>

    <main>
      <article>
        <h2>Publications</h2>
        <h4>NeuroAI</h4>
        <ul class="articles">
          <li>
            <a class="paper_title" href="assets/cosyne24_poster.pdf"
              >Continual learning using dendritic modulations on view-invariant
              feedforward weights</a
            >
            <label for="sn-cosyne24" class="sidenote-toggle">⊕</label>
            <input type="checkbox" id="sn-cosyne24" class="sidenote-toggle" />
            <span class="sidenote">
              <figure>
                <img
                  src="figures/cosyne24.svg"
                  alt="Training the whole network to solve a classification task
                  leads to neural collapse, hindering continual learning (left).
                  Instead, we suggest a separate self-supervised training
                  objective to learn view-invariant features (right), upon which
                  task-specific modulations might induce linearly separable
                  representations."
                />
                <figcaption>
                  Training the whole network to solve a classification task
                  leads to neural collapse, hindering continual learning (left).
                  Instead, we suggest a separate self-supervised training
                  objective to learn view-invariant features (right), upon which
                  task-specific modulations might induce linearly separable
                  representations.
                </figcaption>
              </figure>
            </span>
            <span class="paper_authors"
              ><b>Viet Anh Khoa Tran</b>, Willem A. M. Wybo, Emre O.
              Neftci</span
            >
            <span class="paper_venue"> COSYNE 2024 </span>
            <p>
              The brain can learn continuously without forgetting past skills,
              unlike traditional machine learning models that struggle with
              continual learning. A common solution is to freeze a pretrained
              network and train task-specific readouts, but this often leads to
              suboptimal performance because it assumes the frozen
              representations are linearly separable under the new task.
            </p>
            <p>
              Meanwhile, prior work suggests that in biologic brains, dendritic
              top-down modulations provide a powerful mechanism to solve complex
              tasks while initial feedforward weights solely extract generic
              view-invariant features. This view aligns with the ‘neural
              collapse’ phenomenon from supervised machine learning, as the
              optimal solution for such algorithms is to be invariant to
              task-irrelevant features that are potentially relevant for other
              tasks. Instead, we posit that feature extraction can be learned
              solely by optimizing the networks to attract representations of
              smoothly moving visual stimuli, akin to contrastive
              self-supervised learning methods.
            </p>
            <p>
              We propose a continual learner that optimizes the feedforward
              weights towards view-invariant representations while training
              task-specific modulations in a supervised manner towards separable
              class clusters, which we train in a standard task-incremental
              setting. We show that this simple approach avoids catastrophic
              forgetting of class clusters, as opposed to training the whole
              network in a supervised manner, while also outperforming (a)
              task-specific readout without modulations and (b) frozen
              feedforward weights. This suggests that (a) top-down modulations
              are necessary and sufficient to shift the representations towards
              separable clusters and that (b) the SSL objective learns novel
              features based on the newly presented objects while maintaining
              features relevant to previous tasks, without requiring specific
              synaptic consolidation mechanisms.
            </p>
          </li>
          <li>
            <a
              class="paper_title"
              href="https://www.pnas.org/doi/full/10.1073/pnas.2300558120"
              >NMDA-driven dendritic modulation enables multitask representation
              learning in hierarchical sensory processing pathways</a
            >
            <span class="paper_authors"
              >Willem A. M. Wybo, Matthias C. Tsai, <b>Viet Anh Khoa Tran</b>,
              Bernd Illing, Jakob Jordan, Abigail Morrison, Walter Senn</span
            >
            <span class="paper_venue">
              Proceedings of the National Academy of Sciences, 120 (32)
            </span>
          </li>
        </ul>
        <h4>Natural Language Processing</h4>
        <ul class="articles">
          <li>
            <a class="paper_title" href="https://arxiv.org/abs/2210.13700"
              >Does Joint Training Really Help Cascaded Speech Translation?</a
            >
            <span class="paper_authors"
              ><b>Viet Anh Khoa Tran</b>, David Thulke, Yingbo Gao, Christian
              Herold, Hermann Ney</span
            >
            <span class="paper_venue"> EMNLP 2022 </span>
          </li>
          <li>
            <a class="paper_title" href="https://arxiv.org/abs/2104.10507"
              >On Sampling-Based Training Criteria for Neural Language
              Modeling</a
            >
            <span class="paper_authors">
              Yingbo Gao, David Thulke, Alexander Gerstenberger,
              <b>Viet Anh Khoa Tran</b>, Ralf Schlüter, Hermann Ney
            </span>
            <span class="paper_venue"> INTERSPEECH 2021 </span>
            <p>
              As the vocabulary size of language models increases, training the
              cross-entropy loss across the entire vocabulary becomes
              computationally expensive. However, it is unclear why certain
              sampling-based training criteria such as noise contrastive
              estimation (NCE) work well in practice compared to others. Here,
              starting from three fundamental criteria, namely mean squared
              error (MSE), binary cross-entropy (BCE), and cross-entropy (CE),
              we explicitly write out sampling-based versions such as importance
              sampling, NCE and standard Monte Carlo sampling and derive a
              'correction term' that - if applied during inference - makes the
              sampling-based training criteria perform similarly to NCE.
            </p>
          </li>

          <li>
            <a
              class="paper_title"
              href="https://aclanthology.org/2019.iwslt-1.20/"
              >Analysis of positional encodings for neural machine
              translation</a
            >
            <label for="sn-iwslt2019" class="sidenote-toggle">⊕</label>
            <input type="checkbox" id="sn-iwslt2019" class="sidenote-toggle" />
            <span class="sidenote">
              <figure>
                <img
                  width="250"
                  src="figures/iwslt2019.svg"
                  alt="Relative Positional Encoding as proposed by Shaw et al., 2019"
                />
                <figcaption>
                  Relative positional encodings as proposed by Shaw et al., 2019
                </figcaption>
              </figure>
            </span>
            <span class="paper_authors">
              Jan Rosendahl, <b>Viet Anh Khoa Tran,</b> Weiyue Wang, Hermann Ney
            </span>
            <span class="paper_venue"> IWSLT 2019 </span>
            <p>
              We show in the context of machine translation that while relative
              positional encodings are not beneficial for performance on
              sequence lengths seen during training, they are crucial for
              generalization to longer sequences. Nowadays, this fact is widely
              acknowledged outside of machine translation (e.g.
              <a href="https://arxiv.org/abs/2108.12284"
                >Csordas, Irie and Schmidhuber, 2021</a
              >) and relative positional encodings are used in many
              state-of-the-art models.
            </p>
          </li>
        </ul>
      </article>
      <article>
        <h2>Side Projects</h2>
        <ul>
          <li>
            <a
              class="bold"
              href="https://github.com/tran-khoa/ANNO1404-Warenrechner-App"
              >ANNO 1404 Supply Chain Calculator </a
            ><br />
            A high school side project to calculate the optimal supply chain in
            ANNO 1404 given the current population as an Android app.
          </li>
          <li>
            <a class="bold" href="https://github.com/lebertran/harvestcraft">
              Porting HarvestCraft to Minecraft 1.9 </a
            ><br />
            With 90M downloads,
            <a
              href="https://www.curseforge.com/minecraft/mc-mods/pams-harvestcraft"
              >Pam's HarvestCraft</a
            >
            is one of the most popular mods for Minecraft. I ported it to
            Minecraft 1.9 which
            <a href="https://www.patreon.com/posts/harvestcraft-1-9-5789042"
              >made it into the official code</a
            >. This is the project that started my journey into programming.
          </li>
        </ul>
      </article>
    </main>
    <script
      src="https://kit.fontawesome.com/39127df8ef.js"
      crossorigin="anonymous"
    ></script>
  </body>
</html>
